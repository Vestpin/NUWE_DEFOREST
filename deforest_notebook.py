# -*- coding: utf-8 -*-
"""Deforest_Notebook.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Vvi3W-_3TRRK33gRfWZptoQTclW6c8ro

# load libs
"""

import numpy as np
import tensorflow as tf
from tensorflow import keras
from google.colab import drive
import pandas as pd
from sklearn.model_selection import train_test_split
from keras_preprocessing.image import ImageDataGenerator
import matplotlib.pyplot as plt
from keras.callbacks import ModelCheckpoint, EarlyStopping
from keras.applications.inception_v3 import InceptionV3 , preprocess_input
from keras.layers import Dense, Flatten
from keras.models import Model , load_model
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report
from sklearn.utils import class_weight
from sklearn.preprocessing import OneHotEncoder, MinMaxScaler
import os
from PIL import Image
from numpy import asarray
from tensorflow.keras.utils import load_img

"""# load data"""

drive.mount('/content/drive')

df1=pd.read_csv('/content/drive/MyDrive/NUWE_DEFOREST/train.csv')

df1[['example_path','set','img']]=df1['example_path'].str.split('/',expand = True)

"""# const"""

IMAGE_SIZE = (299, 299) #Inception was originaly trained on 299,299
BATCH_SIZE = 64
INPUTSHAPE = IMAGE_SIZE + (3,) # add 3 colour format

"""# Create Generators"""

df = df1 # keep original 
df["label"] = df["label"].replace({0: 'Plantation',
                                   1: 'Grassland_Shrubland',
                                   2:'Smallholder_Agriculture'
                                   }) # replace for flow
train_df , validate_df = train_test_split(df , test_size=0.20, random_state=42)
train_df = train_df.reset_index(drop=True)
validate_df = validate_df.reset_index(drop=True)

train_datagen = ImageDataGenerator(
    rotation_range=40,
    shear_range=0.3,
    zoom_range=0.3,
    horizontal_flip=True,
    width_shift_range=0.2,
    preprocessing_function = preprocess_input
) # add noice for generalizability and preproces 

train_generator = train_datagen.flow_from_dataframe(
    train_df, 
    "/content/drive/MyDrive/NUWE_DEFOREST/train", 
    x_col='img', 
    y_col='label',
    target_size=IMAGE_SIZE,
    class_mode='categorical',
    batch_size=BATCH_SIZE,
    shuffle=True,
)# takes mapping from dataframe and onehot encodes, resizes, amd batches

val_generator = train_datagen.flow_from_dataframe(
    validate_df, 
    "/content/drive/MyDrive/NUWE_DEFOREST/train", 
    x_col='img',
    y_col='label',
    target_size=IMAGE_SIZE,
    class_mode='categorical',
    batch_size=BATCH_SIZE,
    shuffle=False
)

train_generator.class_indices

len(train_df)

"""# vizual check gen"""

t_img, label= train_generator.next()

def plotImages(img_np,label=0,num = 5):
  '''
  takes an tuble of arrays and shows num of pics
  as check if loaded correct and fun 
  '''

  for idx , img in enumerate(img_np):
    if label == 0: 

      if idx <= num:

        plt.figure(figsize= (10,10))
        plt.imshow(abs(img))
        plt.title(str(img.shape))
        plt.axis=False
        plt.show()
      
    else:

      if idx <= num:

        plt.figure(figsize= (10,10))
        plt.imshow(abs(img))
        plt.title(str(img.shape)+str(label[idx]))
        plt.axis=False
        plt.show()

plotImages(t_img,label,5)

"""#initiate model """

# initialize model with pretrained weights
base_model = InceptionV3(weights='imagenet',
                         input_shape= INPUTSHAPE,
                         include_top = False)

# to not overwrite trained features
for layer in base_model.layers:
  layer.trainable = False

X = Flatten()(base_model.output) # output raster to array

# several dense layers for decision making 
# no dropout/normalization already a lot in inception model #giveoverfitachance
X = Dense(units = 64 , activation = 'relu')(X) 
X = Dense(units = 32 , activation = 'relu')(X)
X = Dense(units = 32 , activation = 'relu')(X)

# end node for % chance catagory membership 
X = Dense(units = 3 , activation = 'softmax' )(X)

model = Model(base_model.input,X)


model.compile(optimizer= 'adam' ,
              loss = 'categorical_crossentropy',
              metrics='accuracy')

model.summary()

"""# model checkpoint"""

# model checkpoint for easy model save 
modelcheck = ModelCheckpoint('./best_model.h5', 
                             monitor='val_accuracy',
                             verbose=1,save_best_only=True)

# early stop to counter overfit 
early_stop=EarlyStopping(monitor='val_accuracy',
                         min_delta=0.01,
                         patience=8,
                         verbose=1)

callbacks=[modelcheck,early_stop]

# weights to counter imbalance dataset 
class_weight = {0: 3.,
                1: 1.,
                2: 1.2}

steps_epoch=len(train_df)//64

# train model high epoch early stop should trigger 
his = model.fit(train_generator , 
                steps_per_epoch= steps_epoch, 
                epochs= 35 ,
                validation_data = val_generator,
                class_weight = class_weight, 
                callbacks= callbacks)

#info used to determen class_weights
train_df['label'].value_counts()

np.unique(label , axis = 0,return_counts= True)

"""# classification report no finetune"""

# load model for easy start 
loaded_model= load_model('/content/best_model.h5')

# classification seems good but model has difficulty with 0
y_pred = np.argmax(loaded_model.predict(val_generator),axis = 1)
print(classification_report(val_generator.classes, y_pred))

"""#End Predict"""

test_samples = sum([len(files) for _, _, files in os.walk('/content/drive/MyDrive/NUWE_DEFOREST/test')])

test_samples=os.listdir('/content/drive/MyDrive/NUWE_DEFOREST/test')

from PIL import Image
from numpy import asarray
test_samples_array=[]
for i in range(len(test_samples)):
  img = load_img(f'/content/drive/MyDrive/NUWE_DEFOREST/test/{test_samples[i]}',target_size =(299,299))
  numpydata = asarray(img)
  test_samples_array.append(numpydata)
test_samples_array= np.array(test_samples_array)

test_samples_array=np.array(test_samples_array)

test_samples_array.shape

preprocess_input(test_samples_array)

test_processed=preprocess_input(test_samples_array)

plotImages(a)

y_pred = np.argmax(loaded_model.predict(test_processed),axis = 1)

best_guess=pd.DataFrame(y_pred, columns = ['target'])

best_guess.to_json(r'Predictions.json')